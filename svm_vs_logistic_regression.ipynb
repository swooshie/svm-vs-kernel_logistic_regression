{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditya Jhaveri : N13689134 : Assignment-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLSs1Eby_pPk"
   },
   "source": [
    "# The SVM vs. Logistic Regression Showdown\n",
    "\n",
    "In this lab, you will practice working with non-linear kernels combined with logistic regression and SVM classifiers. The goal is to compare these commonly used techniques. Which comes out on top in terms of accuracy? Runtime? Is there much of a difference at all?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Msj5mwJs_pPl"
   },
   "source": [
    "## Loading the Data\n",
    "\n",
    "First, we load all the packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "5TKE1o5c_pPm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "import scipy\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L58cF4Cy_pPn"
   },
   "source": [
    "Again we download the data from the Tensorflow package, which you will need to install.  You can get the data from other sources as well.\n",
    "\n",
    "In the Tensorflow dataset, the training and test data are represented as arrays:\n",
    "\n",
    "     Xtr.shape = 60000 x 28 x 28\n",
    "     Xts.shape = 10000 x 28 x 28\n",
    "     \n",
    "The test data consists of `60000` images of size `28 x 28` pixels; the test data consists of `10000` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "Pigfivuh_pPn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr shape: (60000, 28, 28)\n",
      "Xts shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(Xtr_raw,ytr),(Xts_raw,yts) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print('Xtr shape: %s' % str(Xtr_raw.shape))\n",
    "print('Xts shape: %s' % str(Xts_raw.shape))\n",
    "\n",
    "ntr = Xtr_raw.shape[0]\n",
    "nts = Xts_raw.shape[0]\n",
    "nrow = Xtr_raw.shape[1]\n",
    "ncol = Xtr_raw.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgE5TO4b_pPo"
   },
   "source": [
    "Each pixel value is from `[0,255]`.  For this lab, it will be convenient to recale the value to -1 to 1 and reshape it as a `ntr x npix` and `nts x npix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "ahZI8kazNihC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n",
      " 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
      "  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n",
      "   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n",
      "   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n",
      " 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n",
      "   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n",
      "   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n",
      " 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n",
      "   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
      " 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n",
      " 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n",
      " 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n",
      " 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "[-0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.3         0.12352941  0.49215686  0.12352941 -0.30392157\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.31176471  0.43333333\n",
      "  0.48823529  0.48823529  0.48823529  0.42941176 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.28823529  0.39019608  0.49215686  0.48823529  0.4372549\n",
      "  0.41372549  0.48823529 -0.27647059 -0.47647059 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.46078431 -0.26470588  0.37843137\n",
      "  0.48823529  0.49215686  0.48823529  0.29215686 -0.17058824  0.48823529\n",
      "  0.49215686 -0.02156863 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5         0.13921569  0.48823529  0.48823529  0.48823529  0.49215686\n",
      "  0.48823529  0.48823529 -0.12352941  0.24117647  0.49215686  0.15490196\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.3         0.43333333\n",
      "  0.49215686  0.49215686  0.24509804 -0.05294118  0.49215686  0.39411765\n",
      " -0.31568627 -0.19019608  0.5         0.15882353 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.31176471  0.43333333  0.48823529  0.48823529  0.20196078\n",
      " -0.45294118 -0.20588235 -0.0254902  -0.41764706 -0.5        -0.5\n",
      "  0.49215686  0.45294118 -0.30392157 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.35098039  0.14705882\n",
      "  0.49215686  0.41372549  0.31568627 -0.17058824 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5         0.49215686  0.48823529\n",
      "  0.14705882 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.47254902  0.19803922  0.48823529  0.44117647 -0.22156863\n",
      " -0.4254902  -0.39019608 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5         0.49215686  0.48823529  0.26470588 -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.27647059\n",
      "  0.48823529  0.48823529 -0.25294118 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  0.49215686  0.48823529  0.26470588 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5         0.27647059  0.49215686  0.24509804\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5         0.5         0.49215686\n",
      "  0.26862745 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.20196078  0.46470588  0.48823529 -0.06078431 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5         0.49215686  0.48823529  0.08039216 -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.16666667  0.48823529\n",
      "  0.40196078 -0.40196078 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.47254902  0.02941176\n",
      "  0.49215686  0.22941176 -0.45294118 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.16666667  0.48823529  0.3745098  -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.47254902  0.01372549  0.48823529  0.38235294 -0.22156863\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.16666667  0.48823529  0.06862745 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.31176471  0.14705882\n",
      "  0.48823529  0.17843137 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.1627451   0.49215686\n",
      "  0.38235294 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.05294118  0.43333333  0.49215686  0.13529412 -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.16666667  0.48823529  0.47647059  0.07254902\n",
      " -0.31176471 -0.38627451 -0.16666667  0.19803922  0.38235294  0.49215686\n",
      "  0.3745098   0.15490196 -0.28039216 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.16666667  0.48823529  0.48823529  0.48823529  0.39803922  0.34313725\n",
      "  0.48823529  0.48823529  0.48823529  0.26862745  0.00980392 -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.39019608  0.28039216\n",
      "  0.48823529  0.48823529  0.49215686  0.48823529  0.48823529  0.41372549\n",
      "  0.06862745 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.40196078  0.00196078  0.48823529\n",
      "  0.49215686  0.48823529  0.05294118 -0.35490196 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5       ]\n"
     ]
    }
   ],
   "source": [
    "npix = nrow*ncol\n",
    "Xtr = Xtr_raw.reshape((ntr,npix))\n",
    "print(Xtr[1,:])\n",
    "Xtr = (Xtr/255 - .5)\n",
    "print(Xtr[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "QE-50PdU_pPo"
   },
   "outputs": [],
   "source": [
    "npix = nrow*ncol\n",
    "Xtr = (Xtr_raw/255 - 0.5)\n",
    "Xtr = Xtr.reshape((ntr,npix))\n",
    "\n",
    "Xts = (Xts_raw/255 - 0.5)\n",
    "Xts = Xts.reshape((nts,npix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "LqDWXC9yN6JH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " ...\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "print(Xtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frhehbKs_pPo"
   },
   "source": [
    "For this lab we're only going to use a fraction of the MNIST data -- otherwise our models will take too much time and memory to run. Using only part of the training data will of course lead to worse results. Given enough computational resources and time, we would ideally be running on the full data set. The follow code creates a new test and train set, with 10000 examples for train and 5000 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "cUHbBwL1_pPp"
   },
   "outputs": [],
   "source": [
    "ntr1 = 10000\n",
    "nts1 = 5000\n",
    "Iperm = np.random.permutation(ntr1)\n",
    "Xtr1 = Xtr[Iperm[:ntr1],:]\n",
    "ytr1 = ytr[Iperm[:ntr1]]\n",
    "Iperm = np.random.permutation(nts1)\n",
    "Xts1 = Xts[Iperm[:nts1],:]\n",
    "yts1 = yts[Iperm[:nts1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlz6g-X8_pPq"
   },
   "source": [
    "## Problem set up and establishing a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T90pjHcr_pPq"
   },
   "source": [
    "To simplify the problem (and speed things up) we're also going to restrict to binary classification. In particular, let's try to design classifier a that separates the 8's from all other digits.\n",
    "\n",
    "Create binary 0/1 label vectors `ytr8` and `yts8` which are 1 wherever `ytr1` and `yts1` equal 8, and 0 everywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "q6Flo-KS_pPq"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ytr8 =\n",
    "# yts8 =\n",
    "\n",
    "ytr8 = np.where(ytr1 == 8, 1, 0)\n",
    "yts8 = np.where(yts1 == 8, 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdT2KGyj_pPr"
   },
   "source": [
    "Most of the digits in the test dataset aren't equal to 8. So if we simply guess 0 for every image in `Xts`, we might expect to get classification accuracy around 90%. Our goal should be to significantly beat this **baseline**.\n",
    "\n",
    "Formally, write a few lines of code to check what test error would be achieved by the all zeros classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "3JweYada_pPr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuaracy = 0.9056\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "onesr8 = np.where(ytr8 == 1)\n",
    "acc = (len(ytr8) - onesr8[0].shape[0])/len(ytr8)\n",
    "print('Accuaracy = {0:4}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oz_7u10V_pPr"
   },
   "source": [
    "As a second baseline, let's see how we do with standard (non-kernel) logistic regression. As in the MNIST demo, you can use `scikit-learn`'s built in function `linear_model.LogisticRegression` to fit the model and compute the accuracy. Use no regularization and the `lbfgs` solver. You should acheive an improvement to around 93-95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "oocR_r9C_pPr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuaracy = 0.9344\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# ...\n",
    "# acc =\n",
    "# print('Logistic Regression Accuaracy = {0:f}'.format(acc))\n",
    "\n",
    "model = linear_model.LogisticRegression(solver='lbfgs', C=1e10, max_iter=1000)\n",
    "model.fit(Xtr1, ytr8)\n",
    "\n",
    "y_pred = model.predict(Xts1)\n",
    "acc = (sum(y_pred == yts8))/len(y_pred)\n",
    "print('Logistic Regression Accuaracy = {0:4}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXuJVilo_pPr"
   },
   "source": [
    "## Kernel Logistic Regression\n",
    "\n",
    "To improve on this baseline performance, let's try using the logistic regression classifier with a *non-linear* kernel. Recall from class that any non-linear kernel similarity function $k(\\vec{w},\\vec{z})$ is equal to $\\phi(\\vec{w})^T\\phi(\\vec{z})$ for some feature transformation $\\phi$. However, we typically do not need to compute this feature tranformation explicitly: instead we can work directly with the kernel gram matrix $K \\in \\mathbb{R}^{n\\times n}$. Recall that $K_{i,j} = k(\\vec{x}_i,\\vec{x}_j)$ where $\\vec{x}_i$ is the $i^\\text{th}$ training data point.\n",
    "\n",
    "For this lab we will be using the radial basis function kernel. For a given scaling factor $\\gamma$ this kernel is defined as:\n",
    "$$\n",
    "k(\\vec{w},\\vec{z}) = e^{-\\gamma\\|\\vec{w}-\\vec{z}\\|_2^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "q8znIoWi_pPs"
   },
   "outputs": [],
   "source": [
    "def rbf_kernel(w,z,gamma):\n",
    "    d = w - z\n",
    "    return np.exp(-gamma*np.sum(d*d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WFi15JF_pPs"
   },
   "source": [
    "Construct the kernel matrix `K1` for `Xtr1` with `gamma = .05`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "9PBzA-5__pPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.25304055e-03 1.71321421e-02 ... 2.39769750e-02\n",
      "  6.70109693e-03 2.33623087e-02]\n",
      " [1.25304055e-03 1.00000000e+00 4.10431880e-03 ... 9.54983024e-04\n",
      "  3.05452994e-03 6.94805035e-04]\n",
      " [1.71321421e-02 4.10431880e-03 1.00000000e+00 ... 3.03017041e-02\n",
      "  8.81420192e-03 1.04144868e-02]\n",
      " ...\n",
      " [2.39769750e-02 9.54983024e-04 3.03017041e-02 ... 1.00000000e+00\n",
      "  1.82182295e-02 3.82456754e-03]\n",
      " [6.70109693e-03 3.05452994e-03 8.81420192e-03 ... 1.82182295e-02\n",
      "  1.00000000e+00 4.14650517e-03]\n",
      " [2.33623087e-02 6.94805035e-04 1.04144868e-02 ... 3.82456754e-03\n",
      "  4.14650517e-03 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "gamma = .05\n",
    "K1 = np.zeros((Xtr1.shape[0], Xtr1.shape[0]))\n",
    "for i, row in enumerate(K1):\n",
    "    for j, col in enumerate(K1.T):\n",
    "        K1[i, j] = rbf_kernel(Xtr1[i, :], Xtr1[j, :], gamma)\n",
    "\n",
    "print(K1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wB_X-JOR_pPs"
   },
   "source": [
    "If you used a for loop (which is fine) your code might take several minutes to run! Part of the issue is that Python won't know to properly parallize your for loop. For this reason, when constructing kernel matrices it is often faster to us a built-in, carefully optimized function with explicit parallelization. Scikit learn provides such a function through their `metrics` library.\n",
    "\n",
    "Referring to the documentation here\n",
    "https://scikit-learn.org/stable/modules/metrics.html#metrics, use this built in function to recreate the same kernel matrix you did above. Store the result at `K`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "Nhd-xFsn_pPt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.25304055e-03 1.71321421e-02 ... 2.39769750e-02\n",
      "  6.70109693e-03 2.33623087e-02]\n",
      " [1.25304055e-03 1.00000000e+00 4.10431880e-03 ... 9.54983024e-04\n",
      "  3.05452994e-03 6.94805035e-04]\n",
      " [1.71321421e-02 4.10431880e-03 1.00000000e+00 ... 3.03017041e-02\n",
      "  8.81420192e-03 1.04144868e-02]\n",
      " ...\n",
      " [2.39769750e-02 9.54983024e-04 3.03017041e-02 ... 1.00000000e+00\n",
      "  1.82182295e-02 3.82456754e-03]\n",
      " [6.70109693e-03 3.05452994e-03 8.81420192e-03 ... 1.82182295e-02\n",
      "  1.00000000e+00 4.14650517e-03]\n",
      " [2.33623087e-02 6.94805035e-04 1.04144868e-02 ... 3.82456754e-03\n",
      "  4.14650517e-03 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from sklearn.metrics.pairwise import rbf_kernel as rbf_kernel_sk\n",
    "K = rbf_kernel_sk(Xtr1, Xtr1, gamma=gamma)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO9CnBbN_pPt"
   },
   "source": [
    "Check that you used the function correctly by writing code to confirm that `K = K1`, or at least that the two are equal up to very small differences (which could arise due to numerical precision issues). Try to do this **without a for loop** so that the equality check takes advantage of paralellism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "SCOfrRRD_pPt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the two kernel matrices equal (within a small tolerance)? True\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "are_equal = np.allclose(K, K1, atol=1e-8)\n",
    "\n",
    "print(\"Are the two kernel matrices equal (within a small tolerance)?\", are_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrIT1xJL_pPt"
   },
   "source": [
    "When using a non-linear kernel, it is important to check that you have chosen reasonable parameters (in our case the only parameter is `gamma`). We typically do not want $k(\\vec{x}_i,\\vec{x}_j)$ to be either negligably small, or very large for all $i\\neq j$ in our data set, or we won't be able to learn anything. For the RBF kernel this means that, for any $\\vec{x}_i$ we don't want $k(\\vec{x}_i,\\vec{x}_j)$ very close to 1 (e.g. .9999) for all $j$, or very close to $0$ (e.g. 1e-8) for all $j$.\n",
    "\n",
    "Let's just check that we're in good shape for the first data vector $\\vec{x}_0$. Do so by printing out the 10 largest and 10 smallest values of $k(\\vec{x}_0,\\vec{x}_j)$ for $j\\neq 0$. Note that we always have $k(\\vec{x}_0,\\vec{x}_0) = 1$ for the RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "z2yZ-vdt_pPt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum similarities: [0.1441433  0.14471591 0.14665544 0.150195   0.15054071 0.15245242\n",
      " 0.15304697 0.15974273 0.16782655 0.16923569]\n",
      "\n",
      "Minimum similarities: [2.40521253e-05 2.98295584e-05 3.38176572e-05 3.77959453e-05\n",
      " 3.88328604e-05 3.90582979e-05 4.27346464e-05 5.53763081e-05\n",
      " 6.61567736e-05 7.14055509e-05]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "xK10 = K1[0]\n",
    "K1_without_xK10 = np.delete(xK10, 0)\n",
    "\n",
    "K10_largest_values = np.sort(K1_without_xK10)[-10:]\n",
    "K10_smallest_values = np.sort(K1_without_xK10)[:10]\n",
    "\n",
    "print(f\"Maximum similarities: {K10_largest_values}\\n\")\n",
    "print(f\"Minimum similarities: {K10_smallest_values}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0Vfxdfu_pPu"
   },
   "source": [
    "### Implementation\n",
    "Maybe surprisingly Scikit learn does not have an implementation for kernel logistic regression. So we have to implement our own!\n",
    "\n",
    "Write a function function `log_fit` that minimizes the $\\ell_2$-regularized logisitic regression loss:\n",
    "$$\n",
    "L(\\boldsymbol{\\alpha}) =\\sum_{i=1}^n (1-y_i)(\\phi(\\mathbf{x}_i)^T\\phi(\\mathbf{X})^T\\vec{\\alpha}) - \\log(h(\\phi(\\mathbf{x}_i)^T\\phi(\\mathbf{X})^T\\boldsymbol{\\alpha}) + \\lambda \\|\\phi(\\mathbf{X})^T\\boldsymbol{\\alpha}\\|_2^2.\n",
    "$$\n",
    "As input it takes an $n\\times n$ kernel matrix $K$ for the training data, an $n$ length vector `y` of binary class labels, and regularization parameter `lamb`.\n",
    "\n",
    "To implement your function you can either use your own implementation of gradient descent or used a built in minimizer from `scipy.optimize.minimize`. I recommend trying the later approach. You could try using e.g. the same L-BGFG method we used earlier. In either case, you will need to write a function to compute the gradient of the logistic regression loss above. Most of the methods in `scipy.optimize.minimize` require a function that computes the gradient as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_fit(K, y, lamb, displayBoolean=True):\n",
    "    n = K.shape[0]\n",
    "    alpha0 = np.zeros(n)\n",
    "\n",
    "    def loss(alpha, K, y, lamb):\n",
    "        f = K @ alpha\n",
    "        h = 1 / (1 + np.exp(-f))\n",
    "        return -np.sum(y * np.log(h + 1e-8) + (1 - y) * np.log(1 - h + 1e-8)) + lamb * np.dot(alpha, alpha)\n",
    "    \n",
    "    def gradient(alpha, K, y, lamb):\n",
    "        f = K @ alpha\n",
    "        h = 1 / (1 + np.exp(-f))\n",
    "        grad = K.T @ (h - y) + 2 * lamb * alpha\n",
    "        return grad\n",
    "\n",
    "    result = minimize(loss, alpha0, jac=gradient, args=(K, y, lamb), method='L-BFGS-B', options=\n",
    "                      {'disp': displayBoolean, 'maxiter': 20000})\n",
    "    # if not result.success:\n",
    "    #     print(result.x)\n",
    "    #     raise ValueError(\"Optimization failed:\", result.message)\n",
    "    \n",
    "    return result  # Optimal alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PC4MZYmXMAmY"
   },
   "source": [
    "Use the `log_fit` function defined above to find parameters `alpha` for the kernel logistic regression model using `lamb = 0` and `K` as constructed above (with `gamma = .05`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "bNW1cYbjMBwj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        10000     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D+03    |proj g|=  2.13262D+02\n",
      "\n",
      "At iterate    1    f=  4.05603D+03    |proj g|=  2.47068D+01\n",
      "\n",
      "At iterate    2    f=  3.88698D+03    |proj g|=  2.45440D+01\n",
      "\n",
      "At iterate    3    f=  3.34927D+03    |proj g|=  2.90947D+01\n",
      "\n",
      "At iterate    4    f=  2.87196D+03    |proj g|=  2.81512D+01\n",
      "\n",
      "At iterate    5    f=  2.06306D+03    |proj g|=  1.68426D+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    6    f=  1.51593D+03    |proj g|=  7.98500D+00\n",
      "\n",
      "At iterate    7    f=  1.22331D+03    |proj g|=  3.88072D+00\n",
      "\n",
      "At iterate    8    f=  1.03891D+03    |proj g|=  4.77241D+00\n",
      "\n",
      "At iterate    9    f=  8.86376D+02    |proj g|=  2.91802D+00\n",
      "\n",
      "At iterate   10    f=  7.43484D+02    |proj g|=  3.06586D+00\n",
      "\n",
      "At iterate   11    f=  6.23916D+02    |proj g|=  3.25988D+00\n",
      "\n",
      "At iterate   12    f=  5.59236D+02    |proj g|=  4.14671D+00\n",
      "\n",
      "At iterate   13    f=  5.12522D+02    |proj g|=  2.94616D+00\n",
      "\n",
      "At iterate   14    f=  4.79588D+02    |proj g|=  1.55212D+00\n",
      "\n",
      "At iterate   15    f=  4.64467D+02    |proj g|=  5.36346D+00\n",
      "\n",
      "At iterate   16    f=  4.38293D+02    |proj g|=  1.89475D+00\n",
      "\n",
      "At iterate   17    f=  4.21638D+02    |proj g|=  1.86314D+00\n",
      "\n",
      "At iterate   18    f=  3.63711D+02    |proj g|=  1.61559D+01\n",
      "\n",
      "At iterate   19    f=  3.13879D+02    |proj g|=  1.67204D+00\n",
      "\n",
      "At iterate   20    f=  3.04513D+02    |proj g|=  4.68193D+00\n",
      "\n",
      "At iterate   21    f=  2.90049D+02    |proj g|=  2.18742D+00\n",
      "\n",
      "At iterate   22    f=  2.73731D+02    |proj g|=  2.14138D+00\n",
      "\n",
      "At iterate   23    f=  2.29431D+02    |proj g|=  1.70249D+00\n",
      "\n",
      "At iterate   24    f=  1.88224D+02    |proj g|=  1.30153D+00\n",
      "\n",
      "At iterate   25    f=  1.60396D+02    |proj g|=  2.58708D+00\n",
      "\n",
      "At iterate   26    f=  1.39282D+02    |proj g|=  1.71760D+00\n",
      "\n",
      "At iterate   27    f=  1.29537D+02    |proj g|=  1.47624D+00\n",
      "\n",
      "At iterate   28    f=  1.16362D+02    |proj g|=  1.45532D+00\n",
      "\n",
      "At iterate   29    f=  9.95970D+01    |proj g|=  1.25003D+00\n",
      "\n",
      "At iterate   30    f=  7.41701D+01    |proj g|=  1.30480D+00\n",
      "\n",
      "At iterate   31    f=  6.00556D+01    |proj g|=  1.00605D+00\n",
      "\n",
      "At iterate   32    f=  4.22288D+01    |proj g|=  1.08162D+00\n",
      "\n",
      "At iterate   33    f=  2.89986D+01    |proj g|=  9.62466D-01\n",
      "\n",
      "At iterate   34    f=  9.34693D+00    |proj g|=  6.64504D-01\n",
      "\n",
      "At iterate   35    f=  3.46335D+00    |proj g|=  5.03570D-01\n",
      "\n",
      "At iterate   36    f=  1.31582D+00    |proj g|=  6.98358D-02\n",
      "\n",
      "At iterate   37    f=  9.62666D-01    |proj g|=  6.89194D-02\n",
      "\n",
      "At iterate   38    f=  5.79048D-01    |proj g|=  3.93321D-02\n",
      "\n",
      "At iterate   39    f=  3.61868D-01    |proj g|=  2.66778D-02\n",
      "\n",
      "At iterate   40    f=  2.09444D-01    |proj g|=  1.67913D-02\n",
      "\n",
      "At iterate   41    f=  1.13271D-01    |proj g|=  6.91820D-03\n",
      "\n",
      "At iterate   42    f=  6.36604D-02    |proj g|=  3.89007D-03\n",
      "\n",
      "At iterate   43    f=  3.26715D-02    |proj g|=  1.85641D-03\n",
      "\n",
      "At iterate   44    f=  1.67075D-02    |proj g|=  8.50431D-04\n",
      "\n",
      "At iterate   45    f=  8.42760D-03    |proj g|=  7.58909D-04\n",
      "\n",
      "At iterate   46    f=  3.87380D-03    |proj g|=  2.58892D-04\n",
      "\n",
      "At iterate   47    f=  2.25678D-03    |proj g|=  1.58212D-04\n",
      "\n",
      "At iterate   48    f=  1.09950D-03    |proj g|=  9.83359D-05\n",
      "\n",
      "At iterate   49    f=  4.19473D-04    |proj g|=  2.96700D-05\n",
      "\n",
      "At iterate   50    f=  1.87130D-04    |proj g|=  1.45823D-05\n",
      "\n",
      "At iterate   51    f=  4.00748D-05    |proj g|=  7.50638D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "10000     51     53      1     0     0   7.506D-06   4.007D-05\n",
      "  F =   4.0074771860443951E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "[  1.77986192  -3.83952466  -9.4398637  ...  -0.68562897 -20.1914236\n",
      "  -2.92322449]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "lamb = 0\n",
    "K = K/K.max()\n",
    "result = log_fit(K, ytr8, lamb)\n",
    "alpha = result.x\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZXI3kz2_pPu"
   },
   "source": [
    "Suppose we have a test dataset with $m$ examples $\\vec{w}_1,\\ldots, \\vec{w}_m$. Once we obtain a coefficient vector $\\alpha$, making predictions for any $\\vec{w}_j$ in the test set requires computing:\n",
    "$$\n",
    "{y}_{j} = \\sum_{i=1}^n \\alpha_i \\cdot k(\\vec{w}_{j}, \\vec{x}_i).\n",
    "$$\n",
    "where $\\vec{x}_1, \\ldots \\vec{x}_n$ are our training data vectors. We classify $\\vec{w}_{j}$ in class 0 if ${y}_{j} \\leq 0$ and in class 1 if ${y}_{j} > 0$.\n",
    "\n",
    "This computation can be rewritten in matrix form as follows:\n",
    "$$\n",
    "\\vec{y}_{test} = K_{test}\\vec{\\alpha},\n",
    "$$\n",
    "where $\\vec{y}_{text}$ is an $m$ length vector and $K_{test}$ is a $m\\times n$ matrix whose $(j,i)$ entry is equal to $k(\\vec{w}_{j}, \\vec{x}_i)$. We classify $\\vec{w}_{j}$ in class 0 if $\\vec{y}_{test}[j] \\leq 0$ and in class 1 if $\\vec{y}_{test}[j] > 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6BhIPrH_pPv"
   },
   "source": [
    "Use the `pairwise_kernels` function to construct $K_{test}$. Then make predictions for the test set and evaluate the accuracy of our kernel logistic regression classifier. You should see a pretty substantial lift in accuracy to around $97\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "KA6EwssY_pPv"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "gamma = .05\n",
    "Ktest = pairwise_kernels(Xts1, Xtr1, metric='rbf', gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "GrAoBfvK_pPv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.979800\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "yhat = Ktest @ alpha\n",
    "y_pred = (yhat > 0).astype(int)\n",
    "acc = np.mean(y_pred == yts8)\n",
    "print(\"Test accuracy = %f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2SO0yhM_pPv"
   },
   "source": [
    "## Kernel Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9x1jZnh_pPv"
   },
   "source": [
    "The goal of this lab is to compare Kernel Logistic Regression to Kernel Support Vector machines. Following `demo_mnist_svm.ipynb` create and train an SVM classifier on `Xtr1` and `ytr8` using an RBF kernel with `gamma = .05` (the same value we used for logistic regression above). Use margin parameter `C = 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "7r-c_g2z_pPv"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Create a SVM\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "svcrbf = svm.SVC(probability=False,  kernel=\"rbf\", C=10, gamma=.05,verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]....*.*\n",
      "optimization finished, #iter = 5745\n",
      "obj = -491.710632, rho = -0.762837\n",
      "nSV = 3563, nBSV = 0\n",
      "Total nSV = 3563\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, gamma=0.05, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=10, gamma=0.05, verbose=10)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=10, gamma=0.05, verbose=10)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcrbf.fit(Xtr1, ytr8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hcOkVNj_pPw"
   },
   "source": [
    "Calculate and print the accuracy of the SVM classifier. You should obtain a similar result as for logistic regression: something close to $97\\%$ accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "o5nSyO3J_pPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.980200\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "ysvm = svcrbf.predict(Xts1)\n",
    "acc = np.mean(ysvm == yts8)\n",
    "print(\"Test accuracy = %f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWp9NlMa_pPw"
   },
   "source": [
    "## The Showdown\n",
    "\n",
    "Both SVM classifiers and kernel logisitic regression require tuning parameters to obtain the best possible result. In our setting we will stick with an RBF kernel (although this could be tuned). So we only consider tuning the kernel width parameter `gamma`, as well as the regularization parameter `lamb` for logistic regression, and the margin parameter `C` for SVM. We will choose parameters using for-loops and train-test cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia44LYbc_pPw"
   },
   "source": [
    "Train a logistic regression classifier with **all combinations** of the parameters included below in vectors `gamma` and `lamb`. For each setting of parameters, compute and print:\n",
    "* the test error obtained\n",
    "* the total runtime of classification in seconds (including training time and prediction time)\n",
    "\n",
    "For computing runtime you might want to use the `time()` function from the `time` library, which we already imported ealier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "0cHNr0Kw_pPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 0.002793840771287251\n",
      "        x: [-1.948e+00 -1.009e+01 ... -6.306e+00 -3.442e+00]\n",
      "      nit: 26\n",
      "      jac: [ 4.945e-07  8.201e-07 ...  7.558e-07  3.094e-07]\n",
      "     nfev: 29\n",
      "     njev: 29\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/x88zbv4n0b18f8cg1qcv1my40000gn/T/ipykernel_39334/812275000.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dfLRC = pd.concat([dfLRC, pd.DataFrame([[gamma, lamb, acc, endTime - startTime]], columns=dfLRC.columns)], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 1e-06\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 0.368326091206752\n",
      "        x: [-3.980e+00 -7.842e+00 ... -5.287e+00 -2.489e+00]\n",
      "      nit: 45\n",
      "      jac: [-1.558e-07 -1.892e-08 ...  6.520e-08  4.283e-09]\n",
      "     nfev: 52\n",
      "     njev: 52\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.1 0.0001\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 16.124220382261893\n",
      "        x: [-2.397e+00 -4.908e+00 ... -3.551e+00 -1.689e+00]\n",
      "      nit: 51\n",
      "      jac: [-3.227e-07  9.268e-08 ...  3.655e-07 -4.119e-08]\n",
      "     nfev: 57\n",
      "     njev: 57\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.1 0.01\n",
      "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 441.84140073498486\n",
      "        x: [-9.788e-01 -2.205e+00 ... -1.854e+00 -9.559e-01]\n",
      "      nit: 35\n",
      "      jac: [ 1.175e-05  1.719e-06 ...  2.145e-06 -8.543e-07]\n",
      "     nfev: 38\n",
      "     njev: 38\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.05 0\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 4.007477186044395e-05\n",
      "        x: [ 1.780e+00 -3.840e+00 ... -2.019e+01 -2.923e+00]\n",
      "      nit: 51\n",
      "      jac: [-9.143e-07 -2.283e-08 ... -8.102e-08 -7.979e-07]\n",
      "     nfev: 53\n",
      "     njev: 53\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.05 1e-06\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 0.09306782843770367\n",
      "        x: [-1.716e+00 -9.971e-01 ... -3.267e+00 -4.444e-01]\n",
      "      nit: 181\n",
      "      jac: [-1.581e-06  1.069e-07 ... -1.527e-07 -5.554e-07]\n",
      "     nfev: 200\n",
      "     njev: 200\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.05 0.0001\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 4.09625137583607\n",
      "        x: [-8.063e-01 -6.922e-01 ... -1.989e+00 -2.905e-01]\n",
      "      nit: 240\n",
      "      jac: [-1.282e-06  2.289e-07 ...  1.601e-06 -4.986e-07]\n",
      "     nfev: 267\n",
      "     njev: 267\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.05 0.01\n",
      "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 115.96536484956403\n",
      "        x: [-2.226e-01 -4.081e-01 ... -1.122e+00 -1.613e-01]\n",
      "      nit: 177\n",
      "      jac: [ 5.210e-06  1.149e-05 ...  1.374e-05 -1.002e-05]\n",
      "     nfev: 197\n",
      "     njev: 197\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.02 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/x88zbv4n0b18f8cg1qcv1my40000gn/T/ipykernel_39334/3487590976.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  h = 1 / (1 + np.exp(-f))\n",
      "/var/folders/3y/x88zbv4n0b18f8cg1qcv1my40000gn/T/ipykernel_39334/3487590976.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  h = 1 / (1 + np.exp(-f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: -8.445046415750054e-05\n",
      "        x: [ 8.212e+00  4.055e+00 ... -3.769e+01  1.304e+00]\n",
      "      nit: 281\n",
      "      jac: [-1.023e-06 -5.067e-07 ... -6.409e-07 -7.592e-07]\n",
      "     nfev: 335\n",
      "     njev: 335\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.02 1e-06\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 0.12773230262878757\n",
      "        x: [-2.209e-01  8.269e-01 ... -4.242e+00  3.449e-01]\n",
      "      nit: 1676\n",
      "      jac: [-1.804e-06 -3.349e-06 ... -2.703e-06 -2.243e-06]\n",
      "     nfev: 1992\n",
      "     njev: 1992\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.02 0.0001\n",
      "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 5.093902427824644\n",
      "        x: [ 1.526e-02  5.011e-01 ... -2.433e+00  1.787e-01]\n",
      "      nit: 1985\n",
      "      jac: [ 6.066e-05  1.918e-05 ...  3.502e-05  4.024e-05]\n",
      "     nfev: 2322\n",
      "     njev: 2322\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.02 0.01\n",
      "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 116.45226740476757\n",
      "        x: [ 1.722e-01  1.737e-01 ... -1.152e+00  7.375e-02]\n",
      "      nit: 1089\n",
      "      jac: [ 3.971e-04  1.945e-04 ...  1.948e-04  2.033e-04]\n",
      "     nfev: 1263\n",
      "     njev: 1263\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.01 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/x88zbv4n0b18f8cg1qcv1my40000gn/T/ipykernel_39334/3487590976.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  h = 1 / (1 + np.exp(-f))\n",
      "/var/folders/3y/x88zbv4n0b18f8cg1qcv1my40000gn/T/ipykernel_39334/3487590976.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  h = 1 / (1 + np.exp(-f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: -5.7767183817373735e-05\n",
      "        x: [ 2.316e+02  1.551e+02 ... -7.839e+02 -5.191e+01]\n",
      "      nit: 793\n",
      "      jac: [-8.172e-07  3.733e-07 ... -8.287e-07 -3.323e-06]\n",
      "     nfev: 960\n",
      "     njev: 960\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.01 1e-06\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 0.39277482031178523\n",
      "        x: [ 1.926e+00  1.228e+00 ... -7.928e+00  1.381e+00]\n",
      "      nit: 7635\n",
      "      jac: [-1.032e-06  5.763e-06 ... -2.281e-07 -4.300e-06]\n",
      "     nfev: 9127\n",
      "     njev: 9127\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.01 0.0001\n",
      "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 13.566761657332686\n",
      "        x: [ 1.235e+00  7.922e-01 ... -4.310e+00  6.660e-01]\n",
      "      nit: 6552\n",
      "      jac: [ 4.975e-05  2.306e-05 ... -4.467e-05  1.112e-05]\n",
      "     nfev: 7730\n",
      "     njev: 7730\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.01 0.01\n",
      "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 219.2238230531438\n",
      "        x: [ 5.962e-01  3.183e-01 ... -1.700e+00  1.607e-01]\n",
      "      nit: 2891\n",
      "      jac: [ 4.657e-03  3.242e-03 ...  3.280e-03  4.621e-03]\n",
      "     nfev: 3417\n",
      "     njev: 3417\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.005 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/x88zbv4n0b18f8cg1qcv1my40000gn/T/ipykernel_39334/3487590976.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  h = 1 / (1 + np.exp(-f))\n",
      "/var/folders/3y/x88zbv4n0b18f8cg1qcv1my40000gn/T/ipykernel_39334/3487590976.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  h = 1 / (1 + np.exp(-f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: -8.479714593384305e-05\n",
      "        x: [ 1.317e+04 -2.765e+03 ... -2.097e+04  2.263e+03]\n",
      "      nit: 5371\n",
      "      jac: [-5.580e-06 -5.190e-06 ... -5.883e-06 -5.550e-06]\n",
      "     nfev: 6514\n",
      "     njev: 6514\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.005 1e-06\n",
      "  message: STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT\n",
      "  success: False\n",
      "   status: 1\n",
      "      fun: 1.7938831258852321\n",
      "        x: [ 7.660e+00 -6.943e-01 ... -2.142e+01  3.735e+00]\n",
      "      nit: 12410\n",
      "      jac: [ 1.850e-04  7.700e-05 ...  2.071e-04  2.006e-04]\n",
      "     nfev: 15001\n",
      "     njev: 15001\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.005 0.0001\n",
      "  message: STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT\n",
      "  success: False\n",
      "   status: 1\n",
      "      fun: 47.59912118905323\n",
      "        x: [ 3.796e+00  5.198e-01 ... -9.192e+00  1.542e+00]\n",
      "      nit: 12507\n",
      "      jac: [-2.006e-03 -2.534e-03 ... -3.576e-03 -2.106e-03]\n",
      "     nfev: 15001\n",
      "     njev: 15001\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "0.005 0.01\n",
      "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 433.20256039374374\n",
      "        x: [ 8.660e-01  2.878e-01 ... -2.739e+00  1.386e-01]\n",
      "      nit: 4219\n",
      "      jac: [-6.665e-04 -4.754e-03 ... -1.485e-03 -1.633e-04]\n",
      "     nfev: 5066\n",
      "     njev: 5066\n",
      " hess_inv: <10000x10000 LbfgsInvHessProduct with dtype=float64>\n",
      "For Logistic Regression Classifier:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gamma</th>\n",
       "      <th>Lambda</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>1.493977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>2.246975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9636</td>\n",
       "      <td>2.448165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9612</td>\n",
       "      <td>1.774225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9798</td>\n",
       "      <td>2.277664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>7.265285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>10.249683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9766</td>\n",
       "      <td>7.511417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>12.515805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.9844</td>\n",
       "      <td>69.586644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9846</td>\n",
       "      <td>80.501835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>43.286345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>33.028734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>308.870279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9832</td>\n",
       "      <td>263.103780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9824</td>\n",
       "      <td>116.603513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>222.367436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>515.488629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>509.479392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9758</td>\n",
       "      <td>172.955304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gamma    Lambda  Testing Accuracy  Time Taken\n",
       "0   0.100         0            0.9632    1.493977\n",
       "1   0.100  0.000001            0.9652    2.246975\n",
       "2   0.100    0.0001            0.9636    2.448165\n",
       "3   0.100      0.01            0.9612    1.774225\n",
       "4   0.050         0            0.9798    2.277664\n",
       "5   0.050  0.000001            0.9804    7.265285\n",
       "6   0.050    0.0001            0.9796   10.249683\n",
       "7   0.050      0.01            0.9766    7.511417\n",
       "8   0.020         0            0.9818   12.515805\n",
       "9   0.020  0.000001            0.9844   69.586644\n",
       "10  0.020    0.0001            0.9846   80.501835\n",
       "11  0.020      0.01            0.9840   43.286345\n",
       "12  0.010         0            0.9772   33.028734\n",
       "13  0.010  0.000001            0.9830  308.870279\n",
       "14  0.010    0.0001            0.9832  263.103780\n",
       "15  0.010      0.01            0.9824  116.603513\n",
       "16  0.005         0            0.9724  222.367436\n",
       "17  0.005  0.000001            0.9806  515.488629\n",
       "18  0.005    0.0001            0.9816  509.479392\n",
       "19  0.005      0.01            0.9758  172.955304"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas = [.1, .05,.02,.01,.005]\n",
    "lambs = [0,1e-6,1e-4,1e-2]\n",
    "# TODO\n",
    "# ...\n",
    "\n",
    "dfLRC = pd.DataFrame(columns=['Gamma', 'Lambda', 'Testing Accuracy', 'Time Taken'])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, len(gammas)):\n",
    "    gamma = gammas[i]\n",
    "    for j in range(0, len(lambs)):\n",
    "        lamb = lambs[j]\n",
    "        print(gamma, lamb)\n",
    "        K = rbf_kernel_sk(Xtr1, Xtr1, gamma=gamma)\n",
    "        K = K/K.max()\n",
    "        startTime = time.time()\n",
    "        result = log_fit(K, ytr8, lamb, displayBoolean=False)\n",
    "        print(result)\n",
    "        alpha = result.x\n",
    "        Ktest = pairwise_kernels(Xts1, Xtr1, metric='rbf', gamma=gamma)\n",
    "        yhat = Ktest @ alpha\n",
    "        y_pred = (yhat > 0).astype(int)\n",
    "        acc = np.mean(y_pred == yts8)\n",
    "        endTime = time.time()\n",
    "        dfLRC = pd.concat([dfLRC, pd.DataFrame([[gamma, lamb, acc, endTime - startTime]], columns=dfLRC.columns)], ignore_index=True)\n",
    "\n",
    "print(\"For Logistic Regression Classifier:\")\n",
    "dfLRC\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkcP20AM_pPw"
   },
   "source": [
    "TODO: What was the best test error achieved, and what setting of parameters achieved this error? Was the kernel logistic regression classifier more sensitive to changes in `gamma` or `lamb`? Discuss in 1-3 short sentences below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion\n",
    "\n",
    "From the table above we can make the following observations:\n",
    "\n",
    "- The best test-accuracy achieved by KLR was 98.46% for the parameters [$\\gamma$ = 0.02, $\\lambda$ = 1e-4]\n",
    "- Test-Accuracy was sensitive with changes in $\\gamma$ as it affects the kernal function (RBF)\n",
    "- Small changes in $\\gamma$ had significant influence on performance/accuracy\n",
    "- $\\lambda$ majorly controlled the overfitting and accuracy was affected gradually by it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHxF0MXm_pPw"
   },
   "source": [
    "Now let's do the same thing for the kernel Support Vector Classifier. Train an SVM classifier with **all combinations** of the parameters included below in vectors `gamma` and `C`. For each setting of parameters, compute:\n",
    "* the test error obtained\n",
    "* the total runtime of classification in seconds (including training time and prediction time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "FkPkZGIY_pPx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]*.....*\n",
      "optimization finished, #iter = 5982\n",
      "obj = -18.769570, rho = -0.997746\n",
      "nSV = 6918, nBSV = 949\n",
      "Total nSV = 6918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/x88zbv4n0b18f8cg1qcv1my40000gn/T/ipykernel_39334/3016780687.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dfSVM = pd.concat([dfSVM, pd.DataFrame([[gamma, C, acc, endTime - startTime]], columns=dfSVM.columns)], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM].....*....*\n",
      "optimization finished, #iter = 9470\n",
      "obj = -177.776034, rho = -0.977664\n",
      "nSV = 7843, nBSV = 946\n",
      "Total nSV = 7843\n",
      "[LibSVM].........*...*\n",
      "optimization finished, #iter = 12277\n",
      "obj = -938.426025, rho = -0.808865\n",
      "nSV = 7968, nBSV = 600\n",
      "Total nSV = 7968\n",
      "[LibSVM].........*....*\n",
      "optimization finished, #iter = 13250\n",
      "obj = -1011.405531, rho = -0.752041\n",
      "nSV = 8019, nBSV = 0\n",
      "Total nSV = 8019\n",
      "[LibSVM]*.\n",
      "Warning: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1894\n",
      "obj = -18.444899, rho = -0.995437\n",
      "nSV = 2761, nBSV = 1426\n",
      "Total nSV = 2761\n",
      "[LibSVM].\n",
      "Warning: using -h 0 may be faster\n",
      "*.*\n",
      "optimization finished, #iter = 2624\n",
      "obj = -146.332194, rho = -0.956396\n",
      "nSV = 2975, nBSV = 1339\n",
      "Total nSV = 2975\n",
      "[LibSVM]...*..*\n",
      "optimization finished, #iter = 5222\n",
      "obj = -469.477894, rho = -0.785850\n",
      "nSV = 3417, nBSV = 231\n",
      "Total nSV = 3417\n",
      "[LibSVM]....*.*\n",
      "optimization finished, #iter = 5745\n",
      "obj = -491.710632, rho = -0.762837\n",
      "nSV = 3563, nBSV = 0\n",
      "Total nSV = 3563\n",
      "[LibSVM]*.\n",
      "Warning: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1079\n",
      "obj = -18.019023, rho = -1.023046\n",
      "nSV = 1957, nBSV = 1819\n",
      "Total nSV = 1957\n",
      "[LibSVM].*\n",
      "optimization finished, #iter = 1186\n",
      "obj = -122.377664, rho = -1.174684\n",
      "nSV = 1727, nBSV = 1473\n",
      "Total nSV = 1727\n",
      "[LibSVM].*.*\n",
      "optimization finished, #iter = 2601\n",
      "obj = -448.497714, rho = -1.562882\n",
      "nSV = 1326, nBSV = 394\n",
      "Total nSV = 1326\n",
      "[LibSVM]..*..*\n",
      "optimization finished, #iter = 4151\n",
      "obj = -576.925515, rho = -1.908195\n",
      "nSV = 1409, nBSV = 0\n",
      "Total nSV = 1409\n",
      "[LibSVM]*.\n",
      "Warning: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1055\n",
      "obj = -18.169141, rho = -1.067649\n",
      "nSV = 1930, nBSV = 1846\n",
      "Total nSV = 1930\n",
      "[LibSVM]*.*\n",
      "optimization finished, #iter = 1054\n",
      "obj = -131.613317, rho = -1.555752\n",
      "nSV = 1705, nBSV = 1591\n",
      "Total nSV = 1705\n",
      "[LibSVM].*\n",
      "optimization finished, #iter = 1986\n",
      "obj = -612.671933, rho = -3.140266\n",
      "nSV = 1133, nBSV = 687\n",
      "Total nSV = 1133\n",
      "[LibSVM]...*..*\n",
      "optimization finished, #iter = 5529\n",
      "obj = -1249.551198, rho = -5.444231\n",
      "nSV = 1059, nBSV = 20\n",
      "Total nSV = 1059\n",
      "[LibSVM]*.\n",
      "Warning: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1031\n",
      "obj = -18.447980, rho = -1.078836\n",
      "nSV = 1915, nBSV = 1862\n",
      "Total nSV = 1915\n",
      "[LibSVM].*\n",
      "optimization finished, #iter = 1106\n",
      "obj = -148.606285, rho = -1.724149\n",
      "nSV = 1820, nBSV = 1735\n",
      "Total nSV = 1820\n",
      "[LibSVM].*\n",
      "optimization finished, #iter = 1570\n",
      "obj = -837.582722, rho = -4.507987\n",
      "nSV = 1224, nBSV = 986\n",
      "Total nSV = 1224\n",
      "[LibSVM]....*..*\n",
      "optimization finished, #iter = 6575\n",
      "obj = -2906.093284, rho = -11.277210\n",
      "nSV = 935, nBSV = 210\n",
      "Total nSV = 935\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gamma</th>\n",
       "      <th>C</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9022</td>\n",
       "      <td>18.640530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.9022</td>\n",
       "      <td>29.459618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.9282</td>\n",
       "      <td>35.433605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.9342</td>\n",
       "      <td>41.849773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9022</td>\n",
       "      <td>8.341913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.9228</td>\n",
       "      <td>9.192097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.050</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>10.357674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.050</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>10.820482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9022</td>\n",
       "      <td>5.133613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.9512</td>\n",
       "      <td>4.564300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.020</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>3.549462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.020</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.9858</td>\n",
       "      <td>3.937401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9022</td>\n",
       "      <td>5.092281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>4.523837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.010</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>3.060502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.010</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.9870</td>\n",
       "      <td>2.932722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9022</td>\n",
       "      <td>5.055978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.9320</td>\n",
       "      <td>4.780758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.005</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.9672</td>\n",
       "      <td>3.287903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>2.683586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gamma      C  Testing Accuracy  Time Taken\n",
       "0   0.100   0.01            0.9022   18.640530\n",
       "1   0.100   0.10            0.9022   29.459618\n",
       "2   0.100   1.00            0.9282   35.433605\n",
       "3   0.100  10.00            0.9342   41.849773\n",
       "4   0.050   0.01            0.9022    8.341913\n",
       "5   0.050   0.10            0.9228    9.192097\n",
       "6   0.050   1.00            0.9774   10.357674\n",
       "7   0.050  10.00            0.9802   10.820482\n",
       "8   0.020   0.01            0.9022    5.133613\n",
       "9   0.020   0.10            0.9512    4.564300\n",
       "10  0.020   1.00            0.9820    3.549462\n",
       "11  0.020  10.00            0.9858    3.937401\n",
       "12  0.010   0.01            0.9022    5.092281\n",
       "13  0.010   0.10            0.9450    4.523837\n",
       "14  0.010   1.00            0.9760    3.060502\n",
       "15  0.010  10.00            0.9870    2.932722\n",
       "16  0.005   0.01            0.9022    5.055978\n",
       "17  0.005   0.10            0.9320    4.780758\n",
       "18  0.005   1.00            0.9672    3.287903\n",
       "19  0.005  10.00            0.9838    2.683586"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas = [.1, .05,.02,.01,.005]\n",
    "Cs = [.01,.1,1,10]\n",
    "# TODO\n",
    "\n",
    "# ...\n",
    "\n",
    "dfSVM = pd.DataFrame(columns=['Gamma', 'C', 'Testing Accuracy', 'Time Taken'])\n",
    "\n",
    "for gamma in gammas:\n",
    "    for C in Cs:\n",
    "        svcrbf = svm.SVC(probability=False,  kernel=\"rbf\", C=C, gamma=gamma ,verbose=10)\n",
    "\n",
    "        startTime = time.time()\n",
    "        svcrbf.fit(Xtr1, ytr8)\n",
    "        ysvm = svcrbf.predict(Xts1)\n",
    "        acc = np.mean(yts8 == ysvm)\n",
    "        endTime = time.time()\n",
    "        dfSVM = pd.concat([dfSVM, pd.DataFrame([[gamma, C, acc, endTime - startTime]], columns=dfSVM.columns)], ignore_index=True)\n",
    "\n",
    "dfSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPI-k50w_pPx"
   },
   "source": [
    "TODO: What was the best test error achieved, and what setting of parameters achieved this error? Which performed better in terms of accuracy, the SVM or logisitic regression classifier? How about in terms of runtime? **Add some discussion here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion\n",
    "\n",
    "- In Kernal Logistic Regression, we can see from the table that the best test-accuracy of 98.46% was achieved when $\\gamma$ = 0.02 and $\\lambda$ = 1e-4\n",
    "- In SVM, we can see from the table that the best test accuracy of 98.7% was achieved when $\\gamma$ = 0.01 and $\\lambda$ = 10\n",
    "- In SVM, we can observe that, for the same value of $\\gamma$, the test-accuracy was increasing monotonically as the value of $C$ increased\n",
    "\n",
    "##### Observations\n",
    "- Overall Kernel Logistic Regression was able to maintain a test accuracy of more than 96% meanwhile SVM had varying test accuracy from 90% to 98.7% depending on the values of $\\gamma$ and $\\lambda$\n",
    "- From the column `Time Taken`, we can observe that Kernel Logistic Regression takes significantly more amount of time than SVM, hence states SVM is much faster than Kernel Logistic Regression when the dataset is huge (our test dataset was of size 5000)\n",
    "- KLR is better for significant smaller datasets\n",
    "\n",
    "Finally, considering the computational time and accuracy to be achieved for a given scenario, SVM is to be preferred more than KLR because of the observations made above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wec-DXhg_pPx"
   },
   "source": [
    "**NOTE:** For `sklearns`'s built in classifiers, including svm.SVC, there is a function called `GridSearchCV` which can automatically perform hyperparamater tuning for you. The main advantage of the method (as opposed to writing for-loops) is that it supports parallelization, so it can fit with different parameters at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] END .....................C=0.01, gamma=0.02, kernel=rbf; total time=  11.1s\n",
      "[CV] END .....................C=0.01, gamma=0.05, kernel=rbf; total time=  17.9s\n",
      "[CV] END .....................C=0.01, gamma=0.05, kernel=rbf; total time=  17.9s\n",
      "[CV] END .....................C=0.01, gamma=0.05, kernel=rbf; total time=  18.0s\n",
      "[CV] END .....................C=0.01, gamma=0.05, kernel=rbf; total time=  18.1s\n",
      "[CV] END .....................C=0.01, gamma=0.05, kernel=rbf; total time=  18.1s\n",
      "[CV] END .....................C=0.01, gamma=0.02, kernel=rbf; total time=  10.9s\n",
      "[CV] END .....................C=0.01, gamma=0.01, kernel=rbf; total time=  10.8s\n",
      "[CV] END .....................C=0.01, gamma=0.02, kernel=rbf; total time=  11.0s\n",
      "[CV] END .....................C=0.01, gamma=0.01, kernel=rbf; total time=  10.9s\n",
      "[CV] END .....................C=0.01, gamma=0.02, kernel=rbf; total time=  11.1s\n",
      "[CV] END .....................C=0.01, gamma=0.02, kernel=rbf; total time=  11.3s\n",
      "[CV] END .....................C=0.01, gamma=0.01, kernel=rbf; total time=  10.7s\n",
      "[CV] END ......................C=0.01, gamma=0.1, kernel=rbf; total time=  38.8s\n",
      "[CV] END ......................C=0.01, gamma=0.1, kernel=rbf; total time=  38.8s\n",
      "[CV] END ......................C=0.01, gamma=0.1, kernel=rbf; total time=  38.8s\n",
      "[CV] END ......................C=0.01, gamma=0.1, kernel=rbf; total time=  39.0s\n",
      "[CV] END ......................C=0.01, gamma=0.1, kernel=rbf; total time=  39.1s\n",
      "[CV] END .....................C=0.01, gamma=0.01, kernel=rbf; total time=  10.2s\n",
      "[CV] END ....................C=0.01, gamma=0.005, kernel=rbf; total time=  10.0s\n",
      "[CV] END ....................C=0.01, gamma=0.005, kernel=rbf; total time=  10.1s\n",
      "[CV] END .....................C=0.01, gamma=0.01, kernel=rbf; total time=  10.3s\n",
      "[CV] END ....................C=0.01, gamma=0.005, kernel=rbf; total time=  10.1s\n",
      "[CV] END ....................C=0.01, gamma=0.005, kernel=rbf; total time=  10.0s\n",
      "[CV] END ....................C=0.01, gamma=0.005, kernel=rbf; total time=  10.8s\n",
      "[CV] END ......................C=0.1, gamma=0.05, kernel=rbf; total time=  20.6s\n",
      "[CV] END ......................C=0.1, gamma=0.05, kernel=rbf; total time=  20.6s\n",
      "[CV] END ......................C=0.1, gamma=0.02, kernel=rbf; total time=  10.3s\n",
      "[CV] END ......................C=0.1, gamma=0.05, kernel=rbf; total time=  20.8s\n",
      "[CV] END ......................C=0.1, gamma=0.05, kernel=rbf; total time=  20.8s\n",
      "[CV] END ......................C=0.1, gamma=0.05, kernel=rbf; total time=  20.6s\n",
      "[CV] END ......................C=0.1, gamma=0.02, kernel=rbf; total time=  10.4s\n",
      "[CV] END ......................C=0.1, gamma=0.02, kernel=rbf; total time=  10.4s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  10.4s\n",
      "[CV] END ......................C=0.1, gamma=0.02, kernel=rbf; total time=  10.6s\n",
      "[CV] END ......................C=0.1, gamma=0.02, kernel=rbf; total time=  10.5s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  10.3s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  10.2s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  10.1s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  10.3s\n",
      "[CV] END .....................C=0.1, gamma=0.005, kernel=rbf; total time=  10.7s\n",
      "[CV] END .....................C=0.1, gamma=0.005, kernel=rbf; total time=  10.7s\n",
      "[CV] END .....................C=0.1, gamma=0.005, kernel=rbf; total time=  10.5s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=  47.8s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=  48.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=  47.8s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=  47.9s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=  48.3s\n",
      "[CV] END .....................C=0.1, gamma=0.005, kernel=rbf; total time=  10.7s\n",
      "[CV] END .....................C=0.1, gamma=0.005, kernel=rbf; total time=  10.7s\n",
      "[CV] END ........................C=1, gamma=0.02, kernel=rbf; total time=   8.8s\n",
      "[CV] END ........................C=1, gamma=0.02, kernel=rbf; total time=   8.0s\n",
      "[CV] END ........................C=1, gamma=0.05, kernel=rbf; total time=  24.2s\n",
      "[CV] END ........................C=1, gamma=0.05, kernel=rbf; total time=  24.4s\n",
      "[CV] END ........................C=1, gamma=0.05, kernel=rbf; total time=  24.7s\n",
      "[CV] END ........................C=1, gamma=0.05, kernel=rbf; total time=  24.8s\n",
      "[CV] END ........................C=1, gamma=0.05, kernel=rbf; total time=  24.2s\n",
      "[CV] END ........................C=1, gamma=0.02, kernel=rbf; total time=   7.7s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   6.9s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   6.9s\n",
      "[CV] END ........................C=1, gamma=0.02, kernel=rbf; total time=   8.0s\n",
      "[CV] END ........................C=1, gamma=0.02, kernel=rbf; total time=   8.1s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   6.8s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   6.8s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   7.0s\n",
      "[CV] END .......................C=1, gamma=0.005, kernel=rbf; total time=   7.6s\n",
      "[CV] END .......................C=1, gamma=0.005, kernel=rbf; total time=   7.3s\n",
      "[CV] END .......................C=1, gamma=0.005, kernel=rbf; total time=   7.4s\n",
      "[CV] END .......................C=1, gamma=0.005, kernel=rbf; total time=   7.2s\n",
      "[CV] END .......................C=1, gamma=0.005, kernel=rbf; total time=   7.3s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=  57.7s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=  57.7s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=  57.8s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=  57.2s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=  58.2s\n",
      "[CV] END .......................C=10, gamma=0.02, kernel=rbf; total time=   9.1s\n",
      "[CV] END .......................C=10, gamma=0.05, kernel=rbf; total time=  24.6s\n",
      "[CV] END .......................C=10, gamma=0.02, kernel=rbf; total time=   8.8s\n",
      "[CV] END .......................C=10, gamma=0.05, kernel=rbf; total time=  24.5s\n",
      "[CV] END .......................C=10, gamma=0.02, kernel=rbf; total time=   8.8s\n",
      "[CV] END .......................C=10, gamma=0.05, kernel=rbf; total time=  24.7s\n",
      "[CV] END .......................C=10, gamma=0.05, kernel=rbf; total time=  25.3s\n",
      "[CV] END .......................C=10, gamma=0.05, kernel=rbf; total time=  25.4s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   7.2s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   7.2s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   7.0s\n",
      "[CV] END .......................C=10, gamma=0.02, kernel=rbf; total time=   9.1s\n",
      "[CV] END .......................C=10, gamma=0.02, kernel=rbf; total time=   9.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   7.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   6.9s\n",
      "[CV] END ......................C=10, gamma=0.005, kernel=rbf; total time=   6.2s\n",
      "[CV] END ......................C=10, gamma=0.005, kernel=rbf; total time=   6.4s\n",
      "[CV] END ......................C=10, gamma=0.005, kernel=rbf; total time=   6.1s\n",
      "[CV] END ......................C=10, gamma=0.005, kernel=rbf; total time=   6.2s\n",
      "[CV] END ......................C=10, gamma=0.005, kernel=rbf; total time=   5.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=  55.9s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=  54.5s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=  55.2s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=  55.5s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=  53.1s\n",
      "Best Parameters: {'C': 10, 'gamma': 0.02, 'kernel': 'rbf'}\n",
      "Test Accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'C': [.01,.1,1,10],\n",
    "    'gamma': [.1, .05,.02,.01,.005],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "svcGrid = svm.SVC()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svcGrid, param_grid=params, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search.fit(Xtr1, ytr8)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(Xts1)\n",
    "accuracy = np.mean(yts8 == y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "svm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
